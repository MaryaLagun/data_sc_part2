{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности текста с помощью Python\n",
    "\n",
    "---\n",
    "\n",
    "Эта публикация представляет собой незначительно сокращенный перевод статьи Кайла Стратиса [Use Sentiment Analysis With Python to Classify Movie Reviews](https://realpython.com/sentiment-analysis-python/). Сначала в этой публикации рассматриваются ключевые этапы работы с текстом, скрытые внутри конвейера spaCy, потом на примере практической задачи анализа тональности отзывов на [IMDB](https://ru.wikipedia.org/wiki/Internet_Movie_Database) рассматривается процесс построения соответствующих функций для сентимент-анализа.\n",
    "\n",
    "---\n",
    "\n",
    "[Анализ тональности](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%82%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0) (сентимент-анализ) – инструмент компьютерной лингвистики, оценивающий такую субъективную составляющую текста, как отношение пишущего. Часто это составляет трудность даже для опытных читателей — что уж говорить о программах. Однако эмоциональную окраску текста все-таки можно проанализировать с помощью инструментов экосистемы Python.\n",
    "\n",
    "Зачем это может быть нужно? Существует множество применений для анализа эмоциональной окраски текста. Простой практический пример – такие данные позволяют предсказать поведение биржевых трейдеров относительно конкретной компании по откликам в социальных сетях и другим отзывам.\n",
    "\n",
    "В этом руководстве мы рассмотрим:\n",
    "- в чем заключаются базовые методики [обработки естественного языка](https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0) (англ. natural language processing, NLP);\n",
    "- как машинное обучение может помочь для определения тональности текста;\n",
    "- как применить библиотеку spaCy для создания классификатора анализа настроений.\n",
    "\n",
    "Это руководство предназначено для начинающих практиков машинного обучения.\n",
    "\n",
    "\n",
    "# Предварительная обработка и очистка текстовых данных\n",
    "\n",
    "Любой рабочий процесс анализа данных начинается с их загрузки. Далее мы должны пропустить их через конвейер (pipeline) предобработки:\n",
    "- **токенизировать текст** – разбить текст на предложения, слова и другие единицы;\n",
    "- **удалить стоп-слова** – «если», «но», «или» и т. д.;\n",
    "- **привести слова к нормальной форме**;\n",
    "- **векторизовать тексты** – сделать числовые представления текстов для их дальнейшей обработки классификатором.\n",
    "\n",
    "Все эти шаги служат для уменьшения шума, присущего любому обычному тексту, и повышения точности результатов классификатора. Для решения указанных задач есть несколько отличных библиотек, например, [NLTK](https://www.nltk.org/), [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) и [spaCy](https://spacy.io/). Последнюю мы и будем применять в этом руководстве.\n",
    "\n",
    "Прежде чем идти дальше, убедитесь, что у вас установлена библиотека spaCy и модель для английского языка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# расскомментриуйте две следующие строки,\n",
    "# чтобы установить библиотеку и модуль\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Примечание переводчика**. Использование spaCy для текстов на русском языке подробно описано в README-файле репозитория [spacy-ru](https://github.com/buriy/spacy-ru).\n",
    "\n",
    "---\n",
    "\n",
    "# Токенизация\n",
    "\n",
    "**Токенизация** – это процесс разбиения текста на более мелкие части. В библиотеку spaCy уже встроен конвейер (pipeline), который начинает свою работу по обработке текста с токенизации. В этом руководстве мы разделим текст на отдельные слова. Загрузим пример и проведем разбиение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, as, the, forest, burned, up, on, the, hill, ,, \n",
      ", only, a, few, miles, from, his, house, ., The, car, had, \n",
      ", been, hastily, packed, and, Marta, was, inside, trying, to, round, \n",
      ", up, the, last, of, the, pets, ., \", Where, could, she, be, ?, \", he, wondered, \n",
      ", as, he, continued, to, wait, for, Marta, to, appear, with, the, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы могли заметить, что захваченные токены включают знаки препинания и другие строки, не относящиеся к словам. Это нормальное поведение в случае использований конвейера по умолчанию.\n",
    "\n",
    "# Удаление стоп-слов\n",
    "\n",
    "**Стоп-слова** – это слова, которые могут иметь важное значение в человеческом общении, но не имеют смысла для машин. Библиотека spaCy поставляется со списком стоп-слов по умолчанию (его можно настроить). Проведем фильтрацию полученного списка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, forest, burned, hill, ,, \n",
      ", miles, house, ., car, \n",
      ", hastily, packed, Marta, inside, trying, round, \n",
      ", pets, ., \", ?, \", wondered, \n",
      ", continued, wait, Marta, appear, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной строкой Python-кода мы отфильтровали стоп-слова из токенизированного текста с помощью атрибута токенов `.is_stop`. После удаления стоп-слов список стал короче, исчезли местоимения и служебные слова: артикли, союзы, предлоги и послелоги.\n",
    "\n",
    "# Приведение к нормальной форме\n",
    "В процессе **нормализации** все формы слова приводятся к единому представлению. Например, `watched`, `watching` и `watches` после нормализации превращаются в `watch`. Есть два основных подхода к нормализации: стемминг и лемматизация.\n",
    "\n",
    "В случае **стемминга** выделяется основа слова, дополнив которую можно получить слова-потомки. Такой метод сработает на приведенном примере. Однако это наивный подход – стемминг просто обрезает строку, отбрасывая окончание. Такой метод не обнаружит связь между `feel` и `felt`.\n",
    "\n",
    "**Лемматизация** стремится решить указанную проблему, используя структуру данных, в которой все формы слова связываются с его простейшей формой – леммой. Лемматизация обычно приносит больше пользы, чем стемминг, и потому является единственной стратегией нормализации, предлагаемой spaCy. В рамках NLP-конвейера лемматизация происходит автоматически. Лемма для каждого токена хранится в атрибуте `.lemma_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Token: \\n, lemma: \\n', 'Token: Dave, lemma: Dave', 'Token: watched, lemma: watch', 'Token: forest, lemma: forest', 'Token: burned, lemma: burn', 'Token: hill, lemma: hill', 'Token: ,, lemma: ,', 'Token: \\n, lemma: \\n', 'Token: miles, lemma: mile', 'Token: house, lemma: house', 'Token: ., lemma: .', 'Token: car, lemma: car', 'Token: \\n, lemma: \\n', 'Token: hastily, lemma: hastily', 'Token: packed, lemma: pack', 'Token: Marta, lemma: Marta', 'Token: inside, lemma: inside', 'Token: trying, lemma: try', 'Token: round, lemma: round', 'Token: \\n, lemma: \\n', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \", lemma: \"', 'Token: ?, lemma: ?', 'Token: \", lemma: \"', 'Token: wondered, lemma: wonder', 'Token: \\n, lemma: \\n', 'Token: continued, lemma: continue', 'Token: wait, lemma: wait', 'Token: Marta, lemma: Marta', 'Token: appear, lemma: appear', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \\n, lemma: \\n']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [\n",
    "    f\"Token: {token}, lemma: {token.lemma_}\"\n",
    "    for token in filtered_tokens\n",
    "]\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Примечание**. Обратите внимание на символ подчеркивания в атрибуте `.lemma_`. Это не опечатка, а результат соглашения по именованию в spaCy атрибутов, которые могут быть прочитаны человеком.\n",
    "\n",
    "---\n",
    "\n",
    "Следующим шагом является представление каждого токена способом, понятным машине. Этот процесс называется векторизацией.\n",
    "\n",
    "\n",
    "# Векторизация текста\n",
    "**Векторизация** – преобразование токена в числовой массив, который представляет его свойства. В контексте задачи вектор уникален для каждого токена. Векторные представления токенов используются для оценки сходства слов, классификации текстов и т. д. В spaCy токены векторизуются в виде плотных массивов, в которых для каждой позиции определены ненулевые значений. Это отличает используемый подход от ранних методов, в которых для тех же целей применялись разреженные массивы и большинство позиций были заполнены нулями.\n",
    "\n",
    "Как и другие шаги, векторизация выполняется автоматически в результате вызова `nlp()`. Получим векторное представление одного из токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8371646 ,  1.4529226 , -1.6147211 ,  0.678362  , -0.6594443 ,\n",
       "        1.6417935 ,  0.5796405 ,  2.3021278 , -0.13260496,  0.5750932 ,\n",
       "        1.5654886 , -0.6938864 , -0.59607106, -1.5377437 ,  1.9425622 ,\n",
       "       -2.4552505 ,  1.2321601 ,  1.0434952 , -1.5102385 , -0.5787632 ,\n",
       "        0.12055647,  3.6501784 ,  2.6160972 , -0.5710199 , -1.5221789 ,\n",
       "        0.00629176,  0.22760668, -1.922073  , -1.6252862 , -4.226225  ,\n",
       "       -3.495663  , -3.312053  ,  0.81387717, -0.00677544, -0.11603224,\n",
       "        1.4620426 ,  3.0751472 ,  0.35958546, -0.22527039, -2.743926  ,\n",
       "        1.269633  ,  4.606786  ,  0.34034157, -2.1272311 ,  1.2619178 ,\n",
       "       -4.209798  ,  5.452852  ,  1.6940253 , -2.5972986 ,  0.95049495,\n",
       "       -1.910578  , -2.374927  , -1.4227567 , -2.2528825 , -1.799806  ,\n",
       "        1.607501  ,  2.9914255 ,  2.8065152 , -1.2510269 , -0.54964066,\n",
       "       -0.49980402, -1.3882618 , -0.470479  , -2.9670253 ,  1.7884955 ,\n",
       "        4.5282774 , -1.2602427 , -0.14885521,  1.0419178 , -0.08892632,\n",
       "       -1.138275  ,  2.242618  ,  1.5077229 , -1.5030195 ,  2.528098  ,\n",
       "       -1.6761329 ,  0.16694719,  2.123961  ,  0.02546412,  0.38754445,\n",
       "        0.8911977 , -0.07678384, -2.0690763 , -1.1211847 ,  1.4821006 ,\n",
       "        1.1989193 ,  2.1933236 ,  0.5296372 ,  3.0646474 , -1.7223308 ,\n",
       "       -1.3634219 , -0.47471118, -1.7648507 ,  3.565178  , -2.394205  ,\n",
       "       -1.3800384 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы используем атрибут `.vector` для второго токена в списке `filter_tokens`. В этом наборе это слово `Dave`.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание**. Если для атрибута `.vector` вы получите другой результат, не беспокойтесь. Это может быть связано с тем, что используется другая версия модели `en_core_web_sm` или самой библиотеки `spaCy`.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Использование классификаторов машинного обучения для прогнозирования настроений\n",
    "\n",
    "Теперь текст преобразован в форму, понятную компьютеру, так что мы можем начать работу над его классификацией. Мы рассмотрим три темы, которые дадут общее представление о классификации текстовых данных в результате машинного обучения:\n",
    "\n",
    "1. Инструменты машинного обучения для задач классификации.\n",
    "2. Как происходит классификация.\n",
    "3. Как использовать spaCy для классификации текста.\n",
    "\n",
    "## Инструменты машинного обучения\n",
    "В мире Python есть ряд инструментов для решения задач классификации. Вот некоторые из наиболее популярных:\n",
    "- [TensorFlow](https://www.tensorflow.org/)\n",
    "- [PyTorch](https://pytorch.org/)\n",
    "- [scikit-learn](https://scikit-learn.org/)\n",
    "\n",
    "Этот список не является исчерпывающим, но это наиболее широко используемые фреймворки (библиотеки) машинного обучения, доступные для питонистов. Это мощные инструменты, на освоение и понимание которых уходит много времени. \n",
    "\n",
    "**TensorFlow** разработан Google и является одним из самых популярных фреймворков машинного обучения. Он довольно низкоуровневый, что дает пользователю много возможностей. **PyTorch** – это ответ Facebook на TensorFlow, созданный с учетом незнакомых с машинным обучением Python-разработчиков. **scikit-learn** отличается от TensorFlow и PyTorch, позволяя использовать готовые алгоритмы машинного обучения, не создавая собственных. Он прост в использовании и позволяет быстро обучать классификаторы всего несколькими строками кода.\n",
    "\n",
    "К счастью, spaCy предоставляет довольно простой встроенный классификатор текста. Для начала важно понять общий рабочий процесс любого вида задач классификации:\n",
    "\n",
    "1. Разделяем данные на обучающую и тестовую выборки (наборы данных).\n",
    "2. Выбираем архитектуру модели.\n",
    "3. Используем обучающие данные для настройки параметров модели (этот процесс и называется обучением).\n",
    "4. Используем тестовые данные, чтобы оценить качество обучения модели.\n",
    "5. Используем обученную модель на новых, ранее не рассматривавшихся входных данных для создания прогнозов.\n",
    "\n",
    "Cпециалисты по машинному обучению обычно разделяют набор данных на три составляющих:\n",
    "1. Данные для обучения (training).\n",
    "2. Данные для валидации (validation).\n",
    "3. Данные для теста (test).\n",
    "\n",
    "**Обучающий набор данных**, как следует из названия, используется для обучения модели. **Валидационные данные** используются для настройки гиперпараметров модели и оценки модели непосредственно в процессе обучения. Гиперпараметры контролируют процесс обучения и структуру модели. Такими параметрами являются, например, скорость обучения и размер пакета данных (батчей). Набор гиперпараметров зависит от используемой модели. **Тестовый набор данных** – включает данные, позволяющие судить о конечном качестве работы модели. \n",
    "\n",
    "---\n",
    "\n",
    "**Примечание переводчика**. Фактически в рамках руководства мы ограничимся лишь валидацией модели в процессе ее обучения. Данные для обучения и валидации мы берем лишь из обучающей выборки. В качестве проверки модели мы ограничимся лишь единичным примером. Расширить тестирование не составит труда – тестовый набор данных включен в репозиторий и имеет ту же структуру, что и обучающие данные. В постскриптуме статьи рассмотрен необычный пример процедуры тестирования обученной модели на сторонних данных несколько иной природы.\n",
    "\n",
    "---\n",
    "\n",
    "Теперь, когда мы в общих чертах рассмотрели процесс классификации, пора применить его с помощью spaCy.\n",
    "\n",
    "## Как использовать spaCy для классификации текста\n",
    "Мы уже знаем, что spaCy берет на свои плечи заботы о предварительной обработке текста с помощью конструктора `nlp()`. Конвейер по умолчанию определен в файле `JSON`, связанном с уже существующей моделью (`en_core_web_sm` в этом руководстве) или [моделью, созданной пользвателем](https://spacy.io/usage/processing-pipelines).\n",
    "\n",
    "Один из встроенных компонентов конвейера называется `textcat` (сокр. `TextCategorizer`), который позволяет назначать текстовым данным категории и использовать их в качестве обучающих данных нейронной сети. Чтобы с помощью этого инструмента обучить модель, необходимо выполнить следующие действия:\n",
    "1. Добавить компонент `textcat` в существующий конвейер.\n",
    "2. Добавить  в компонент `textcat` валидные метки (имена категорий).\n",
    "3. Загрузить, перемешать и разделить на части данные, на которые проходит обучение.\n",
    "4. Обучить модель, оценивая каждую итерацию обучения.\n",
    "5. Использовать обученную модель, чтобы предсказать тональность настроений на текстах, не входивших в обучающую выборку.\n",
    "6. При желании: сохранить обученную модель.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание**. Реализацию этих шагов можно посмотреть в [примерах документации spaCy](https://spacy.io/usage/examples#textcat). Это основной способ классификации текста в spaCy, поэтому код проекта во многом похож на эти примеры.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Создаем собственный анализатор тональности текстов\n",
    "В качестве набора данных, на которых будет происходить обучение и проверка модели мы будем использовать набор данных [Large Movie Review](https://ai.stanford.edu/~amaas/data/sentiment/), собранный [Эндрю Маасом](http://www.andrew-maas.net/). \n",
    "\n",
    "---\n",
    "\n",
    "**Примечение переводчика**. Использование базы данных IMDB является стандартным источником для сентимент-анализа. В рецензиях к фильмам сами пользователи уже соотнесли рецензию и оценку фильма – как бы разметили данные, сопоставив блок текста и его категорию.\n",
    "\n",
    "Предлагаемый в пособии набор данных хранится в виде сжатого tar-архива, который можно извлечь на диск непосредственно из Python. Так как датасеты обычно хранятся отдельно от блокнотов Jupyter (чтобы не выгружать их на GitHub), для дальнейшей работы полезно сменить текущую рабочую директорию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# не забудьте изменить путь к директории с архивом\n",
    "# на тот, что используется в вашей системе\n",
    "os.chdir(os.path.relpath('../../../Datasets/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "fname = 'aclImdb_v1.tar.gz'\n",
    "with tarfile.open(fname, \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "    tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Туда же впоследствии мы сохраним обученную модель классификации данных.\n",
    "\n",
    "---\n",
    "\n",
    "## Загрузка и подготовка данных\n",
    "\n",
    "Далее мы считаем, что вы уже извлекли директорию, содержащую набор данных. Разобьем этап загрузки на конкретные шаги:\n",
    "\n",
    "1. Загрузим текст и метки из структуры файлов и каталогов.\n",
    "2. Перемешаем данные.\n",
    "3. Разделим данные на обучающий и тестовый наборы.\n",
    "4. Вернём два набора данных.\n",
    "\n",
    "Этот процесс самодостаточен, поэтому его логично «упаковать» в отдельную функцию:\n",
    "\n",
    "\n",
    "```python\n",
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "```\n",
    "\n",
    "В сигнатуре функции мы используем аннотацию типов Python 3, чтобы было ясно, какие типы аргументов ожидает функция и какой тип она возвращает. Приведенные параметры позволяют определить каталог, в котором хранятся данные (`data_directory`), соотношение обучающих и тестовых данных (`split`) и количество отбираемых записей (`limit`). Далее нам нужно перебрать все файлы в наборе данных и загрузить их данные в список:\n",
    "\n",
    "```python\n",
    "# одиночными символами решетки здесь и далее помечен код,\n",
    "# добавленный или изменившийся в сравнении с предыдущим кодом\n",
    "import os                                                     #\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0) -> tuple:\n",
    "    # Загрузка данных из файлов\n",
    "    reviews = []                                              #\n",
    "    for label in [\"pos\", \"neg\"]:                              #\n",
    "        labeled_directory = f\"{data_directory}/{label}\"       #\n",
    "        for review in os.listdir(labeled_directory):          #\n",
    "            if review.endswith(\".txt\"):                       #\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()                           #\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")     #\n",
    "                    if text.strip():                          #\n",
    "                        spacy_label = {                       #\n",
    "                            \"cats\": {                         #\n",
    "                                \"pos\": \"pos\" == label,        #\n",
    "                                \"neg\": \"neg\" == label         #\n",
    "                            }                                 #\n",
    "                        }                                     #\n",
    "                        reviews.append((text, spacy_label))   #\n",
    "```\n",
    "\n",
    "Хотя это может показаться сложным, здесь мы просто создаем структуру каталогов данных, ищем и открываем текстовые файлы, а затем добавляем кортеж содержимого и словарь меток в список рецензий `reviews`. Оформление меток в виде словаря – это формат, используемый моделями spaCy во время обучения.\n",
    "\n",
    "Поскольку на этом этапе мы открываем каждый обзор, здесь же полезно заменить HTML-теги `<br />` символами новой строки и использовать строковый метод `.strip ()` для удаления начальных и конечных пробелов.\n",
    "\n",
    "В этом проекте мы не будем сразу удалять стоп-слова из обучающей выборки – это может изменить значение предложения или фразы и снизить предсказательную силу классификатора.\n",
    "\n",
    "После загрузки файлов мы хотим их перетасовать, чтобы устранить любое влияние, обусловленное порядком загрузки обучающих данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random                                  #\n",
    "\n",
    "def load_training_data(\n",
    "    data_directory: str = \"aclImdb/train\",\n",
    "    split: float = 0.8,\n",
    "    limit: int = 0\n",
    ") -> tuple:\n",
    "    # Загрузка данных из файлов\n",
    "    reviews = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        labeled_directory = f\"{data_directory}/{label}\"\n",
    "        for review in os.listdir(labeled_directory):\n",
    "            if review.endswith(\".txt\"):\n",
    "                with open(f\"{labeled_directory}/{review}\") as f:\n",
    "                    text = f.read()\n",
    "                    text = text.replace(\"<br />\", \"\\n\\n\")\n",
    "                    if text.strip():\n",
    "                        spacy_label = {\n",
    "                            \"cats\": {\n",
    "                                \"pos\": \"pos\" == label,\n",
    "                                \"neg\": \"neg\" == label}\n",
    "                        }\n",
    "                        reviews.append((text, spacy_label))\n",
    "    random.shuffle(reviews)                    #\n",
    "\n",
    "    if limit:                                  #\n",
    "        reviews = reviews[:limit]              #\n",
    "    split = int(len(reviews) * split)          #\n",
    "    return reviews[:split], reviews[split:]    #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В добавленных строках кода мы перемешали записи из данных с помощью вызова `random.shuffle()`. Затем мы разбиваем и разделяем данные. Наконец, возвращаем два списка обзоров.\n",
    "\n",
    "Выведем пример записи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HOLLOW MAN is one of the better horror films of the past decade. The sub-plot is original and the main plot is even better. The special effects are brilliant and possibly the best I have ever seen in a horror film. Kevin Bacon proves again that he can handle any role that comes his way.\\n\\n\\n\\nClaude Rains shocked the world with THE INVISIBLE MAN in 1933, well now, Kevin Bacon has shocked *us* with HOLLOW MAN. One of the most thrilling horror films ever. The action is intense and the chills are true. You may actually find yourself jumping if you are watching it in the dark on a stormy night. The supporting cast includes Elizabeth Shue, Josh Brolin, Kim Dickens, Joey Slotnick, Greg Grunberg, and Mary Randle. All of whom do an exceptional job. \\n\\n\\n\\n---SPOILERS---\\n\\n\\n\\nDr. Sebastian Caine (Kevin Bacon) and his team have discovered the secret to making someone invisible. After animal testings, they move on to human testing. But someone has to be the subject. Volenteering, Caine is turned invisible. But when his team is unable to bring back into visibility, Caine is driven mad by his condition as he seeks his revenge...*end spoilers*\\n\\n\\n\\nThe film has created memorable shock sequences and is destined to become a classic well into the next century. Becoming the basis for a spoof joke in SCARY MOVIE 2, this film grabs you by the throat and never lets go. The first 45 minutes or so are slow, developing the characters and showing how their experiments work. The second half is exciting and appealing to most action and horror fans. Think of DEEP BLUE SEA. Then change the sharks into an crazy invisible man. And then change the water into fire and explosions. A rehashing of a killer shark movie. Interesting... HOLLOW MAN gets 5/5.',\n",
       " {'cats': {'pos': True, 'neg': False}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_training_data(\n",
    "    data_directory = \"aclImdb/train\",\n",
    "    split = 0.8,\n",
    "    limit = 0)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Примечание**. Создатели spaCy также выпустили пакет под названием `thinc` ([репозиторий GitHub](https://github.com/explosion/thinc)), который, помимо других функций, включает упрощенный доступ к большим наборам данных, в том числе датасет обзоров IMDB, используемый в этом проекте.\n",
    "\n",
    "---\n",
    "\n",
    "## Обучение классификатора\n",
    "Конвейер spaCy позволяет создать и обучить [сверточную нейронную сеть](https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C) (CNN) для классификации текстовых данных. Здесь этот подход используется для сентимент-анализа, но ничто не мешает распространить его и на другие задачи классификации текстов.\n",
    "\n",
    "В этой части проекта мы выполним три шага:\n",
    "1. Измененим базовый конвейер spaCy для включения компонента `textcat`.\n",
    "2. Создадим цикл для обучения компонента `textcat`.\n",
    "3. Научимся оценивать прогресс обучения модели после заданного количества циклов обучения.\n",
    "\n",
    "### Изменение конвейера `spaCy` для включения `textcat`\n",
    "Загрузим тот же конвейер, что и в примерах в начале руководства, далее добавим компонент `textcat`. После этого укажем `textcat` метки, которые используются в данных: `pos` для положительных отзывов и `neg` для негативных:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20\n",
    ") -> None:\n",
    "    # Строим конвейер\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "```\n",
    "\n",
    "Если вы уже видели [пример textcat](https://spacy.io/usage/examples#textcat) из документации spaCy, то этот код будет вам знаком. Сначала мы загружаем встроенный конвейер `en_core_web_sm`, затем проверяем атрибут `.pipe_names`, чтобы узнать, доступен ли компонент `textcat`. Если это не так, создаем компонент с помощью метода `.create_pipe()`, передаем словарь с конфигурацией. Соответствующие инструкции описаны в [документации TextCategorizer](https://spacy.io/api/textcategorizer#init). Наконец, с помощью `.add_pipe()` добавляем компонент в конвейер, последний аргумент указывает, что этот компонент должен быть добавлен в конец конвейера.\n",
    "\n",
    "Теперь нужно обработать случай, когда компонент `textcat` уже доступен. Добавляем метки:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20\n",
    ") -> None:\n",
    "    # Строим конвейер\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:                                   #\n",
    "        textcat = nlp.get_pipe(\"textcat\")   #\n",
    "\n",
    "    textcat.add_label(\"pos\")                #\n",
    "    textcat.add_label(\"neg\")                #\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Пишем цикл обучения `textcat`\n",
    "Чтобы начать цикл обучения, настраиваем конвейер на обучение компонента `textcat`, генерируем для него пакеты данных с помощью функций из пакета `spacy.util` – `minibatch()` и `compounding()`. Под пакетом данных, батчем (англ. batch) понимается просто та небольшая часть данных, которая участвует в обучении. Пакетная обработка данных позволяет сократить объем памяти, используемый во время обучения и быстрее обновлять гиперпараметры.\n",
    "\n",
    "---\n",
    "**Примечание**. Динамический размер батча (compounding) – относительно новая методика методов машинного обучения, которая должна приводить к ускорению процесса обучения. О ней можно прочитать в [советах по обучению spaCy](https://spacy.io/usage/training#tips-batch-size).\n",
    "\n",
    "---\n",
    "\n",
    "Реализуем описанный цикл обучения, добавив необходимые строки:\n",
    "\n",
    "\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding  #\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20\n",
    ") -> None:\n",
    "    # Строим конвейер\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    # Обучаем только textcat\n",
    "    training_excluded_pipes = [                               #\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"  #\n",
    "    ]                                                         #\n",
    "    with nlp.disable_pipes(training_excluded_pipes):          #\n",
    "        optimizer = nlp.begin_training()                      #\n",
    "        # Итерация обучения                           \n",
    "        print(\"Начинаем обучение\")                            #\n",
    "        batch_sizes = compounding(                            #\n",
    "            4.0, 32.0, 1.001                                  #\n",
    "        )  # Генератор бесконечной последовательности входных чисел\n",
    "```\n",
    "\n",
    "В последних строчках функции создаем список компонентов в конвейере, которые не являются компонентами `textcat`. Далее используем диспетчер контекста `nlp.disable()`, чтобы отключить эти компоненты для всего кода в области действия диспетчера контекста. \n",
    "\n",
    "Далее мы вызываем функцию `nlp.begin_training()`, которая возвращает начальную функцию оптимизатора. Это то, что `nlp.update()` будет впоследствии использовать для обновления весов базовой модели. Затем используем функцию `compounding()` для создания генератора, дающего последовательность значений `batch_sizes`, которые в дальнейшем будут приниматься функцией `minibatch()`.\n",
    "\n",
    "Теперь добавим обучение на батчах:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding  #\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20\n",
    ") -> None:\n",
    "    # Строим конвейер\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    # Обучаем только textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        print(\"Начинаем обучение\")\n",
    "        batch_sizes = compounding(\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # Генератор бесконечной последовательности входных чисел\n",
    "        for i in range(iterations):                           #\n",
    "            loss = {}                                         #\n",
    "            random.shuffle(training_data)                     #\n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:                             #\n",
    "                text, labels = zip(*batch)                    #\n",
    "                nlp.update(                                   #\n",
    "                    text,                                     #\n",
    "                    labels,                                   #\n",
    "                    drop=0.2,                                 #  \n",
    "                    sgd=optimizer,                            #\n",
    "                    losses=loss                               #\n",
    "                )                                             #\n",
    "```\n",
    "\n",
    "Теперь для каждой итерации, указанной в сигнатуре `train_model()`, мы создаем пустой словарь с именем `loss`, который будет обновляться и использоваться функцией `nlp.update()`. Перетасовываем обучающие данные и разделяем их на пакеты разного размера с помощью функции `minibatch()`.\n",
    "\n",
    "Для каждого пакета отделяем текст (`text`) от меток (`labels`) и передаем их в оптимизатор `nlp.update()`. Это фактиески запускает обучение.\n",
    "\n",
    "Параметр `dropout` сообщает `nlp.update()`, какую часть обучающих данных в этом пакете нужно пропустить. Это делается для того, чтобы модели было сложнее переобучиться – запомнить обучающие данные без создания обобщающей модели.\n",
    "\n",
    "### Оценка прогресса обучения модели\n",
    "\n",
    "Поскольку мы будем выполнять множество оценок на большом количестве вычислений, имеет смысл написать отдельную функцию `evaluate_model()`. В этой функции мы будем классифицировать тексты из валидационного набора данных на недообученной модели и сравнивать результаты модели с метками исходных данных.\n",
    "\n",
    "Используя эту информацию, мы вычислим следующие метрики:\n",
    "\n",
    "- **Истинно положительные** (true positives, $TP$) – число отзывов, которые модель правильно предсказала как положительные.\n",
    "- **Ложноположительные** (false positives, $FP$) – число отзывов, которые модель неверно предсказала как положительные, хотя на самом деле они были негативными.\n",
    "- **Истинно отрицательные** (true negatives, $TN$) – число отзывов, которые модель правильно предсказала как негативные.\n",
    "- **Ложноотрицательные** (false negatives, $FN$) – число отзывов, которые модель неверно предсказала как негативные, хотя на самом деле они были положительными.\n",
    "\n",
    "Поскольку наша модель для каждой метки возвращает оценку от 0 до 1, мы определяем положительный или отрицательный результат на основе этой оценки. На основе четырех описанных статистических данных мы вычисляем две метрики: точность и полноту. Эти метрики являются показателями эффективности модели классификации:\n",
    "- **Точность** (precision) – отношение истинно положительных результатов ко всем элементам, отмеченным моделью как положительные (истинные и ложные срабатывания). Точность 1.0 означает, что каждый отзыв, отмеченный нашей моделью как положительный, действительно относится к положительному классу.\n",
    "- **Полнота** (recall) – это отношение истинно положительных отзывов ко всем фактическим положительным отзывам, то есть количество истинно положительных отзывов, деленных на суммарное количество истинно положительных и ложноотрицательных отзывов.\n",
    "\n",
    "$$precision = \\dfrac{TP}{TP + FP},$$\n",
    "\n",
    "$$recall = \\dfrac{TP}{TP + FN}.$$\n",
    "\n",
    "Ещё более популярной метрикой является F1-мера – среднее гармоническое точности и полноты. Максимизация F1-меры приводит к одновременной максимизации этих двух критериев:\n",
    "\n",
    "$$F_1 = \\dfrac{2 \\cdot precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "---\n",
    "**Примечание переводчика**. Подробнее о метриках машинного обучения можно прочитать в [статье Александра Дьяконова](https://dyakonov.org/2019/05/31/%d1%84%d1%83%d0%bd%d0%ba%d1%86%d0%b8%d0%be%d0%bd%d0%b0%d0%bb%d1%8b-%d0%ba%d0%b0%d1%87%d0%b5%d1%81%d1%82%d0%b2%d0%b0-%d0%b2-%d0%b7%d0%b0%d0%b4%d0%b0%d1%87%d0%b5-%d0%b1%d0%b8%d0%bd%d0%b0%d1%80%d0%bd/).\n",
    "\n",
    "---\n",
    "\n",
    "В `evaluate_model()` необходимо передать токенизатор, `textcat` и тестовый набор данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(tokenizer, textcat, test_data: list) -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    # Указываем TP как малое число, чтобы в знаменателе\n",
    "    # не оказался 0\n",
    "    TP, FP, TN, FN = 1e-8, 0, 0, 0\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i]['cats']\n",
    "        score_pos = review.cats['pos'] \n",
    "        if true_label['pos']:\n",
    "            if score_pos >= 0.5:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        else:\n",
    "            if score_pos >= 0.5:\n",
    "                FP += 1\n",
    "            else:\n",
    "                TN += 1    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой функции мы разделяем обзоры и их метки, затем используем выражение-генератор для токенизации каждого из обзоров, подготавливая их для передачи в `textcat`. Выражение-генератор позволяет перебирать токенизированные обзоры, не сохраняя каждый из них в памяти.\n",
    "\n",
    "Затем мы используем `score` и `true_label` для определения ложных и истинных срабатываний модели, которые далее подставляем для расчета точности, полноты и F-меры.\n",
    "\n",
    "Вызовем `evaluate_model()` из описанной ранее функции `train_model()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int = 20) -> None:\n",
    "    # Строим конвейер\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"architecture\": \"simple_cnn\"}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label(\"pos\")\n",
    "    textcat.add_label(\"neg\")\n",
    "\n",
    "    # Обучаем только textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print(\"Начинаем обучение\")\n",
    "        print(\"Loss\\t\\tPrec.\\tRec.\\tF-score\")          #\n",
    "        batch_sizes = compounding(\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # Генератор бесконечной последовательности входных чисел\n",
    "        for i in range(iterations):\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(\n",
    "                    text,\n",
    "                    labels,\n",
    "                    drop=0.2,\n",
    "                    sgd=optimizer,\n",
    "                    losses=loss\n",
    "                )\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(   #\n",
    "                    tokenizer=nlp.tokenizer,           #\n",
    "                    textcat=textcat,                   #\n",
    "                    test_data=test_data                #\n",
    "                )                                      #\n",
    "                print(f\"{loss['textcat']:9.6f}\\t\\\n",
    "{evaluation_results['precision']:.3f}\\t\\\n",
    "{evaluation_results['recall']:.3f}\\t\\\n",
    "{evaluation_results['f-score']:.3f}\")\n",
    "                \n",
    "    # Сохраняем модель                                 #\n",
    "    with nlp.use_params(optimizer.averages):           #\n",
    "        nlp.to_disk(\"model_artifacts\")                 #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы добавили несколько вызовов `print()`, чтобы помочь организовать вывод от функции `evaluate_model()`, которую вызываем в контекстном менеджере  `.use_param ()`, чтобы оценить модель в ее текущем состоянии. \n",
    "\n",
    "После завершения процесса обучения сохраняем только что обученную модель в каталоге с именем `model_artifacts`:\n",
    "\n",
    "Итак, мы получили функцию, которая обучает модель, отображает оценку обучения и позволяет сохранить результат. Произведем ее обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение\n",
      "Loss\t\tPrec.\tRec.\tF-score\n",
      "13.758302\t0.809\t0.776\t0.792\n",
      " 1.080611\t0.827\t0.784\t0.805\n",
      " 0.264118\t0.833\t0.776\t0.804\n",
      " 0.092023\t0.840\t0.764\t0.800\n",
      " 0.037383\t0.834\t0.760\t0.795\n",
      " 0.016112\t0.836\t0.760\t0.796\n",
      " 0.009953\t0.835\t0.766\t0.799\n",
      " 0.006168\t0.830\t0.770\t0.799\n",
      " 0.005931\t0.830\t0.770\t0.799\n",
      " 0.005302\t0.833\t0.776\t0.804\n"
     ]
    }
   ],
   "source": [
    "train, test = load_training_data(limit=20000)\n",
    "train_model(train, test, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание**. Время обучения зависит от используемой системы. Его можно сократить, уменьшив размер выборки (`limit`), однако в этом случае есть риск получить менее точную модель.\n",
    "\n",
    "По мере обучения модели вы будете видеть, как меняются показатели потерь, точности, полноты и F-меры для каждой итерации обучения. Значение функции потерь стремительно уменьшается с каждой итерацией. Остальные параметры также должны меняться, но не так значительно: обычно они растут на самых первых итерациях, а после этого держатся примерно на одном уровне.\n",
    "\n",
    "![image.png](https://files.realpython.com/media/loss_chart.c0fecc2d5c49.png)\n",
    "\n",
    "*Характерное изменение значения функции потерь от числа итерации*\n",
    "\n",
    "\n",
    "### Классифицируем обзоры\n",
    "Теперь у нас есть обученная модель, пора протестировать ее на реальных обзорах – насколько успешно она справится с оценкой их эмоциональной окраски. Для целей этого проекта мы оценим лишь один обзор, вместо которого вы можете подставить любые иные строковые значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "TEST_REVIEW = \"\"\"\n",
    "Transcendently beautiful in moments outside the office, it seems almost\n",
    "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
    "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
    "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
    "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим текст обзора модели, чтобы сгенерировать прогноз и отобразить его пользователю:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_model(input_data: str):\n",
    "    # Загружаем сохраненную модель\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    # Определяем возвращаемое предсказание\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"Положительный отзыв\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"Негативный отзыв\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    print(f\"Текст обзора: {input_data}\\n\\\n",
    "Предсказание: {prediction}\\n\\\n",
    "Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом коде мы передаем входные данные в загруженную модель и генерируем предсказание в атрибуте `cats` переменной `parsed_text`. Затем проверяем оценки каждого настроения и сохраняем метки с более высоким прогнозом. Проверим, что модель корректно отрабатывает на отдельном примере."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст обзора: \n",
      "Transcendently beautiful in moments outside the office, it seems almost\n",
      "sitcom-like in those scenes. When Toni Colette walks out and ponders\n",
      "life silently, it's gorgeous.<br /><br />The movie doesn't seem to decide\n",
      "whether it's slapstick, farce, magical realism, or drama, but the best of it\n",
      "doesn't matter. (The worst is sort of tedious - like Office Space with less humor.)\n",
      "\n",
      "Предсказание: Положительный отзыв\n",
      "Score: 0.612\n"
     ]
    }
   ],
   "source": [
    "test_model(input_data=TEST_REVIEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отзыв действительно несет положительную оценку. Параметр `Score` служит характеристикой уверенности модели. Проверьте поведение функции `test_model()` на других строковых значениях.\n",
    "\n",
    "Итак, мы создали ряд независимых функций `load_data()`, `train_model()`,  `evaluate_model()` и `test_model()`, которые, вместе взятые, будут загружать данные и обучать, оценивать, сохранять и тестировать классификатор анализа эмоциональной окраски текста в Python. Чтобы объединить их вместе в одном файле достаточно, использовать стандартную инструкцию `if __name__ == \"__main__\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     train, test = load_training_data(limit=2500)\n",
    "#     train_model(train, test)\n",
    "#     print(\"Testing model\")\n",
    "#     test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Заключение\n",
    "\n",
    "Поздравляем! Вы обучили модель анализа настроений, используя методы обработки естественного языка.\n",
    "\n",
    "В этом руководстве мы рассмотрели:\n",
    "- базовые методики предобработки текста;\n",
    "- инструменты создания классификаторов;\n",
    "- процесс создания NLP-конвейера с помощью spaCy.\n",
    "\n",
    "Оттолкнувшись от этого проекта, вы можете разработать другие решения. Вот несколько идей, с которых можно начать его расширение:\n",
    "\n",
    "- Во время работы функции `load_data()` текст каждого отзыва загружается в память. Можно ли сделать процесс более эффективным, используя вместо этого функции-генераторы?\n",
    "- Перепишите  код, чтобы удалить стоп-слова во время предобработки или загрузки данных. Как меняется качество и производительность модели?\n",
    "- Используйте библиотеку [click](https://click.palletsprojects.com/en/7.x/), чтобы создать интерактивный интерфейс командной строки.\n",
    "- Изучите конфигурационные параметры для компонента конвейера `textcat` и поэкспериментируйте с различными настройками.\n",
    "\n",
    "Множество различных идей также можно почерпнуть в других [блокнотах Jupyter](https://github.com/matyushkin/lessons).\n",
    "\n",
    "\n",
    "# P.S. Кинопоиск\n",
    "Напоследок проверим модель на аналогичных отзывах на русском языке. Для этого можно было бы использовать русскоязычную модель spaCy (см. примечание выше) и заново обучить модель на приведенных далее примерах, но ради интереса мы воспользуемся моделью, уже обученной на рецензиях IMDB. Для этого протестируем модель на датасете из 3000 записей, собранных с Кинопоиска [Денисом Кудрявцевым](https://habr.com/ru/post/467081/). Выборка сбалансирована: содержится примерно по одной тысяче положительных, негативных и нейтральных отзывов (в датасете IMDB присутствуют только положительные и негативные отзывы). Для удобства работы мы преобразовали набор текстовых файлов в [единый csv-файл](https://github.com/matyushkin/lessons/blob/master/nlp/nlp_datasets/kinopoisk.zip) и перевели колонку рецензий с помощью машинного перевода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>translation</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2530</th>\n",
       "      <td>Великая сила - общественное мнение. Наверное, ...</td>\n",
       "      <td>Public opinion is a great power. It must be ha...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>Когда я шёл на этот фильм, меня терзали сомнен...</td>\n",
       "      <td>When I went to this film, I was tormented by d...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>Рассуждая о новой версии «Красавицы и чудовища...</td>\n",
       "      <td>When discussing the new version of Beauty and ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>Старая добрая франшиза совершенно не собираетс...</td>\n",
       "      <td>The good old franchise is not going to let go ...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>Игры разума,тот фильм который зацепил меня по ...</td>\n",
       "      <td>A Beautiful Mind, the movie that got me hooked...</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review  \\\n",
       "2530  Великая сила - общественное мнение. Наверное, ...   \n",
       "2060  Когда я шёл на этот фильм, меня терзали сомнен...   \n",
       "2032  Рассуждая о новой версии «Красавицы и чудовища...   \n",
       "1510  Старая добрая франшиза совершенно не собираетс...   \n",
       "1196  Игры разума,тот фильм который зацепил меня по ...   \n",
       "\n",
       "                                            translation     type  \n",
       "2530  Public opinion is a great power. It must be ha...  neutral  \n",
       "2060  When I went to this film, I was tormented by d...  neutral  \n",
       "2032  When discussing the new version of Beauty and ...  neutral  \n",
       "1510  The good old franchise is not going to let go ...      bad  \n",
       "1196  A Beautiful Mind, the movie that got me hooked...      bad  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "df = pd.read_csv(\"kinopoisk.zip\", index_col=0)\n",
    "df.sample(frac=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Адаптируем функцию `test_model` для обновленной задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_model(input_data):\n",
    "    loaded_model = spacy.load(\"model_artifacts\")\n",
    "    parsed_text = loaded_model(input_data)\n",
    "    if parsed_text.cats[\"pos\"] > parsed_text.cats[\"neg\"]:\n",
    "        prediction = \"good\"\n",
    "        score = parsed_text.cats[\"pos\"]\n",
    "    else:\n",
    "        prediction = \"bad\"\n",
    "        score = parsed_text.cats[\"neg\"]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pred = df.translation.apply(test_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:8682: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "/home/leo/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3267: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрика accuracy составила: 0.693.\n",
      "F1-мера модели равна 0.735.\n"
     ]
    }
   ],
   "source": [
    "df['pred'] = pred\n",
    "yy = df.iloc[:1999]\n",
    "y_true = yy['type']\n",
    "y_pred = yy['pred']\n",
    "\n",
    "y_true[y_true == 'good'] = 1\n",
    "y_true[y_true == 'bad'] = 0\n",
    "y_pred[y_pred == 'good'] = 1\n",
    "y_pred[y_pred == 'bad'] = 0\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_true.astype(int), y_pred.astype(int))\n",
    "f_score = sklearn.metrics.f1_score(y_true.astype(int), y_pred.astype(int))\n",
    "\n",
    "print(f\"Метрика accuracy составила: {accuracy:.3f}.\")\n",
    "print(f\"F1-мера модели равна {f_score:.3f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Несмотря на то что часть смыслового содержания скрадывается в результате машинного перевода с русского на английский, модель всё равно достаточно хорошо определяет тональность текстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good    559\n",
       "bad     441\n",
       "Name: pred, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['type'] == 'neutral']['pred'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нейтральных записей мы действительно получили один порядок числа положительных и негативных предсказаний – модель сомневается в разделении, ведь в ней не происходило обучения на нейтральных записях."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
